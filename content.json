{"meta":{"title":"Master","subtitle":null,"description":null,"author":"Alan Kim","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"docker1","slug":"docker1-1","date":"2018-08-13T04:40:12.000Z","updated":"2018-08-13T08:08:33.265Z","comments":true,"path":"2018/08/13/docker1-1/","link":"","permalink":"http://yoursite.com/2018/08/13/docker1-1/","excerpt":"","text":"Installation and Configuration Version 차이점사전 준비yum repository 를 업데이트하기 위한 Utility 등을 사용하기 리눅스 device mapper 및 lvm2 1[root@automan1 /]# yum install -y yum-utils device-mapper-persistent-data lvm2 docker repository 추가 1[root@automan1 /]# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo docker 인스톨 1[root@automan1 /]# yum install docker-ce docker service start and check status 123456789101112131415161718192021[root@test1 ~]# systemctl enable docker &amp;&amp; systemctl start docker &amp;&amp; systemctl status docker● docker.service - Docker Application Container Engine Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled) Active: active (running) since Wed 2018-01-03 23:53:59 UTC; 6min ago Docs: https://docs.docker.com Main PID: 1105 (dockerd) CGroup: /system.slice/docker.service ├─1105 /usr/bin/dockerd └─1129 docker-containerd --config /var/run/docker/containerd/containerd.tom...Jan 03 23:53:58 automan1.mylabserver.com dockerd[1105]: time=\"2018-01-03T23:53:58.416...\"Jan 03 23:53:58 automan1.mylabserver.com dockerd[1105]: time=\"2018-01-03T23:53:58.438...\"Jan 03 23:53:58 automan1.mylabserver.com dockerd[1105]: time=\"2018-01-03T23:53:58.496...\"Jan 03 23:53:58 automan1.mylabserver.com dockerd[1105]: time=\"2018-01-03T23:53:58.499...\"Jan 03 23:53:59 automan1.mylabserver.com dockerd[1105]: time=\"2018-01-03T23:53:59.386...\"Jan 03 23:53:59 automan1.mylabserver.com dockerd[1105]: time=\"2018-01-03T23:53:59.477...\"Jan 03 23:53:59 automan1.mylabserver.com dockerd[1105]: time=\"2018-01-03T23:53:59.563...eJan 03 23:53:59 automan1.mylabserver.com dockerd[1105]: time=\"2018-01-03T23:53:59.564...\"Jan 03 23:53:59 automan1.mylabserver.com dockerd[1105]: time=\"2018-01-03T23:53:59.575...\"Jan 03 23:53:59 automan1.mylabserver.com systemd[1]: Started Docker Application Conta....Hint: Some lines were ellipsized, use -l to show in full. 일반 사용자에게 권한 부여하기 12345678910111213[user@test1 ~]$ docker imagesGot permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.35/images/json: dial unix /var/run/docker.sock: connect: permission denied[user@test1 ~]$ sudo ls -al /var/run/docker.sock[sudo] password for user:srw-rw----. 1 root docker 0 Jan 3 23:53 /var/run/docker.sock[user@test1 ~]$ sudo usermod -aG docker user[user@test1 ~]$ exit[user@test1 ~]$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE Ubuntu에서 실행하기관련 1user@test2:~$ sudo apt-get install apt-transport-https ca-certificates curl software-properties-common repository 추가 123user@test2:~$ sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot;user@test2:~$ sudo apt-get update docker installation 1user@test2:~$ sudo apt-get install docker-ce 일반 사용자에게 권한 부여 하기 12345678user@test2:~$ sudo ls -al /var/run/docker.socksrw-rw---- 1 root docker 0 Jan 3 23:54 /var/run/docker.sockuser@test2:~$ sudo usermod -aG docker useruser@test2:~$ exituser@test2:~$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE Selecting Storage DriverStorage Driver를 바꾸면 기존에 다운로드 되어 있던 image들은 모두 날라가게 되니 조심 할것. Configuring Logging Drivers (Syslog, JSON-File, etc)현재 default Logging Driver 확인 1234[root@test1 docker]# docker system info | grep &quot;Logging&quot;WARNING: devicemapper: usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.Logging Driver: json-file 현재는 Json-file인데 logging server는 system log로 변경 12345678910111213141516[root@test1 docker]# vi daemon.json&#123; &quot;storage-driver&quot;: &quot;devicemapper&quot;, &quot;log-driver&quot;: &quot;syslog&quot;, &quot;log-opts&quot;: &#123; &quot;labels&quot;: &quot;status&quot;, &quot;env&quot;: &quot;production&quot; &#125;&#125;[root@test1 docker]# systemctl restart docker[root@test1 docker]# docker system info | grep &quot;Logging&quot;WARNING: devicemapper: usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.Logging Driver: syslog docker image 생성 1234567891011121314151617181920212223242526272829303132333435[root@test1 docker]# docker pull httpdUsing default tag: latestlatest: Pulling from library/httpdf49cf87b52c1: Pull complete02ca099fb6cd: Pull completede7acb18da57: Pull complete770c8edb393d: Pull complete0e252730aeae: Pull complete6e6ca341873f: Pull complete2daffd0a6144: Pull completeDigest: sha256:b5f21641a9d7bbb59dc94fb6a663c43fbf3f56270ce7c7d51801ac74d2e70046Status: Downloaded newer image for httpd:latest[root@test1 docker]# docker iamgesdocker: &apos;iamges&apos; is not a docker command.See &apos;docker --help&apos;[root@test1 docker]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEhttpd latest 7239615c0645 3 weeks ago 177MBhello-world latest f2a91732366c 6 weeks ago 1.85kB[root@test1 docker]# docker run -d httpd638884c6269c37ddaecff1ea75e4f0c2f71991a32a741946c14c6f8686248de9[root@test1 docker]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES638884c6269c httpd &quot;httpd-foreground&quot; 8 seconds ago Up 6 seconds 80/tcp wizardly_poitras[root@test1 docker]# docker container inspect wizardly_poitras | grep IPAdd &quot;SecondaryIPAddresses&quot;: null, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;,[root@test1 docker]# docker container logs wizardly_poitrasError response from daemon: configured logging driver does not support reading log를 확인해보자 먼저 default option인 jsonfile를 포멧을 확인한다. 123456789101112131415161718192021[root@test1 docker]# docker stop wizardly_poitras[root@test1 docker]# docker run -d --log-driver json-file httpd[root@test1 docker]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES502c3ec26f84 httpd &quot;httpd-foreground&quot; 5 minutes ago Up 5 minutes 80/tcp reverent_mayer[root@test1 docker]# docker container inspect reverent_mayer | grep IPAdd &quot;SecondaryIPAddresses&quot;: null, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;,[root@test1 docker]# lynx http://172.17.0.2[root@test1 docker]# docker container logs reverent_mayerAH00558: httpd: Could not reliably determine the server&apos;s fully qualified domain name, using 172.17.0.2. Set the &apos;ServerName&apos; directive globally to suppress this messageAH00558: httpd: Could not reliably determine the server&apos;s fully qualified domain name, using 172.17.0.2. Set the &apos;ServerName&apos; directive globally to suppress this message[Sun Jan 07 05:40:49.649582 2018] [mpm_event:notice] [pid 1:tid 140716881528704] AH00489: Apache/2.4.29 (Unix) configured -- resuming normal operations[Sun Jan 07 05:40:49.654325 2018] [core:notice] [pid 1:tid 140716881528704] AH00094: Command line: &apos;httpd -D FOREGROUND&apos;172.17.0.1 - - [07/Jan/2018:05:43:36 +0000] &quot;GET / HTTP/1.0&quot; 200 45 logging driver를 json에서 syslog로 바꿔보자 현재 Syslog를 사용할 수 있도록 환경 설정을 한다. 아래 두가지 옵션을 활성화 하자 1234567891011121314151617181920212223[root@automan1 ~]# cd /etc[root@automan1 etc]# vim rsyslog.conf$ModLoad imudp$UDPServerRun 514[root@automan1 etc]# systemctl start rsyslog[root@automan1 etc]# systemctl status rsyslog● rsyslog.service - System Logging Service Loaded: loaded (/usr/lib/systemd/system/rsyslog.service; enabled; vendor preset: enabled) Active: active (running) since Sat 2018-06-30 05:12:10 UTC; 1h 5min ago Docs: man:rsyslogd(8) http://www.rsyslog.com/doc/ Main PID: 1072 (rsyslogd) Tasks: 3 Memory: 76.0K CGroup: /system.slice/rsyslog.service └─1072 /usr/sbin/rsyslogd -nJun 30 05:12:09 automan1.mylabserver.com systemd[1]: Starting System Logging Service...Jun 30 05:12:10 automan1.mylabserver.com rsyslogd[1072]: [origin software=&quot;rsyslogd&quot; s...tJun 30 05:12:10 automan1.mylabserver.com systemd[1]: Started System Logging Service.Hint: Some lines were ellipsized, use -l to show in full. syslog를 활용 할 수 있도록 docker 환경 설정을 한다. 1234567891011121314[root@automan1 docker]# vim daemon.json&#123; &quot;storage-driver&quot;: &quot;devicemapper&quot;, &quot;log-driver&quot;: &quot;syslog&quot;, &quot;log-opts&quot;: &#123; &quot;syslog-address&quot;:&quot;udp://localhost:514&quot; &#125;&#125;[root@automan1 docker]# docker info | grep LoggingWARNING: devicemapper: usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.Logging Driver: syslog 이상태에서 local 혹은 remote 머신의 syslog를 구동해보자 12345678[root@automan1 docker]# echo &quot;&quot; &gt; /var/log/messages[root@automan1 docker]# systemctl restart rsyslog[root@automan1 docker]# tail -f /var/log/messagesJun 30 06:27:53 automan1 systemd: Stopping System Logging Service...Jun 30 06:27:53 automan1 rsyslogd: [origin software=&quot;rsyslogd&quot; swVersion=&quot;8.24.0&quot; x-pid=&quot;1072&quot; x-info=&quot;http://www.rsyslog.com&quot;] exiting on signal 15.Jun 30 06:27:53 automan1 systemd: Starting System Logging Service...Jun 30 06:27:53 automan1 rsyslogd: [origin software=&quot;rsyslogd&quot; swVersion=&quot;8.24.0&quot; x-pid=&quot;6636&quot; x-info=&quot;http://www.rsyslog.com&quot;] start 이 상태에서 터미널을 하나 더 열어서 docker image를 하나 구동 해보자 12[user@automan1 ~]$ docker run -d --name=testweb -p 80:80 httpdd4d5e3ff2f1a3b27618fc1cc10f86a6c456f96611fd66dc4d12f2eab760671d2 syslog에 docker 이미지 실행에 대한 로그를 확인 할 수 있다. 123456789101112131415161718192021222324252627282930Jun 30 06:30:25 automan1 kernel: XFS (dm-1): Mounting V4 FilesystemJun 30 06:30:25 automan1 kernel: XFS (dm-1): Ending clean mountJun 30 06:30:25 automan1 kernel: XFS (dm-1): Unmounting FilesystemJun 30 06:30:25 automan1 kernel: XFS (dm-1): Mounting V4 FilesystemJun 30 06:30:25 automan1 kernel: XFS (dm-1): Ending clean mountJun 30 06:30:25 automan1 kernel: XFS (dm-1): Unmounting FilesystemJun 30 06:30:25 automan1 kernel: XFS (dm-1): Mounting V4 FilesystemJun 30 06:30:25 automan1 kernel: XFS (dm-1): Ending clean mountJun 30 06:30:25 automan1 kernel: docker0: port 1(veth2463cb6) entered blocking stateJun 30 06:30:25 automan1 kernel: docker0: port 1(veth2463cb6) entered disabled stateJun 30 06:30:26 automan1 kernel: device veth2463cb6 entered promiscuous modeJun 30 06:30:26 automan1 kernel: IPv6: ADDRCONF(NETDEV_UP): veth2463cb6: link is not readyJun 30 06:30:26 automan1 kernel: docker0: port 1(veth2463cb6) entered blocking stateJun 30 06:30:26 automan1 kernel: docker0: port 1(veth2463cb6) entered forwarding stateJun 30 06:30:26 automan1 kernel: docker0: port 1(veth2463cb6) entered disabled stateJun 30 06:30:26 automan1 NetworkManager[664]: &lt;info&gt; [1530340226.0424] manager: (vethf399284): new Veth device (/org/freedesktop/NetworkManager/Devices/7)Jun 30 06:30:26 automan1 NetworkManager[664]: &lt;info&gt; [1530340226.0457] manager: (veth2463cb6): new Veth device (/org/freedesktop/NetworkManager/Devices/8)Jun 30 06:30:26 automan1 dockerd: time=&quot;2018-06-30T06:30:26Z&quot; level=info msg=&quot;shim docker-containerd-shim started&quot; address=&quot;/containerd-shim/moby/d4d5e3ff2f1a3b27618fc1cc10f86a6c456f96611fd66dc4d12f2eab760671d2/shim.sock&quot; debug=false module=&quot;containerd/tasks&quot; pid=7087Jun 30 06:30:26 automan1 avahi-daemon[592]: Withdrawing workstation service for vethf399284.Jun 30 06:30:26 automan1 kernel: IPv6: ADDRCONF(NETDEV_CHANGE): veth2463cb6: link becomes readyJun 30 06:30:26 automan1 kernel: docker0: port 1(veth2463cb6) entered blocking stateJun 30 06:30:26 automan1 kernel: docker0: port 1(veth2463cb6) entered forwarding stateJun 30 06:30:26 automan1 NetworkManager[664]: &lt;info&gt; [1530340226.2206] device (veth2463cb6): carrier: link connectedJun 30 06:30:26 automan1 NetworkManager[664]: &lt;info&gt; [1530340226.2374] device (docker0): carrier: link connectedJun 30 06:30:26 localhost d4d5e3ff2f1a[6293]: AH00558: httpd: Could not reliably determine the server&apos;s fully qualified domain name, using 172.17.0.2. Set the &apos;ServerName&apos; directive globally to suppress this messageJun 30 06:30:26 localhost d4d5e3ff2f1a[6293]: AH00558: httpd: Could not reliably determine the server&apos;s fully qualified domain name, using 172.17.0.2. Set the &apos;ServerName&apos; directive globally to suppress this messageJun 30 06:30:26 localhost d4d5e3ff2f1a[6293]: [Sat Jun 30 06:30:26.323447 2018] [mpm_event:notice] [pid 1:tid 140124613404544] AH00489: Apache/2.4.33 (Unix) configured -- resuming normal operationsJun 30 06:30:26 localhost d4d5e3ff2f1a[6293]: [Sat Jun 30 06:30:26.329674 2018] [core:notice] [pid 1:tid 140124613404544] AH00094: Command line: &apos;httpd -D FOREGROUND&apos;Jun 30 06:30:28 automan1 avahi-daemon[592]: Registering new address record for fe80::10eb:61ff:fe31:d742 on veth2463cb6.*.Jun 30 06:30:46 automan1 amazon-ssm-agent: 2018-06-30 06:30:46 INFO [HealthCheck] HealthCheck reporting agent health. 로그를 통해 ‘httpd -D FOREGROUND’ 와 같이 foregroud로 실행된 것을 알 수 있다. 이제 간단하게 웹사이트에 접속해보고 log에 어떻게 찍히는지 확이해보자 1234567[user@automan1 ~]$ curl http://localhost&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;[user@automan1 ~]$ curl http://localhost&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;Jun 30 06:38:12 localhost d4d5e3ff2f1a[6293]: 172.17.0.1 - - [30/Jun/2018:06:38:12 +0000] &quot;GET / HTTP/1.1&quot; 200 45Jun 30 06:38:21 localhost d4d5e3ff2f1a[6293]: 172.17.0.1 - - [30/Jun/2018:06:38:21 +0000] &quot;GET / HTTP/1.1&quot; 200 45 만일 특정 컨테이너에 대해 Syslog가 아닌 json 포멧으로 로그를 남기고 싶다면 아래와 같이 컨테이너 실행시 logging option을 지정할 수 있다. 1234567[root@automan1 docker]# docker run -d --name testjson --log-driver json-file httpd[root@automan1 docker]# docker logs testjsonAH00558: httpd: Could not reliably determine the server&apos;s fully qualified domain name, using 172.17.0.2. Set the &apos;ServerName&apos; directive globally to suppress this messageAH00558: httpd: Could not reliably determine the server&apos;s fully qualified domain name, using 172.17.0.2. Set the &apos;ServerName&apos; directive globally to suppress this message[Sat Jun 30 06:43:33.764925 2018] [mpm_event:notice] [pid 1:tid 140063915788160] AH00489: Apache/2.4.33 (Unix) configured -- resuming normal operations[Sat Jun 30 06:43:33.771181 2018] [core:notice] [pid 1:tid 140063915788160] AH00094: Command line: &apos;httpd -D FOREGROUND&apos; Setup swarm, configure managers, add nodes, and setup backup scheduleConfigure Manager기존에 다운로드한 이미지를 모두 삭제 1234567[root@test1 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEhttpd latest 7239615c0645 3 weeks ago 177MBhello-world latest f2a91732366c 6 weeks ago 1.85kB[root@test1 ~]# docker rmi 7239615c0645[root@test1 ~]# docker rmi f2a91732366c Docker Swarm에 대해서는 추후에 자세히 살펴보고, 우선 아래와 같이 설치를 해보자 설치는 Swarm을 수행할 서버의 IP address를 지정하면 된다. 12345678[root@test1 ~]# docker swarm init --advertise-addr 172.31.22.17Swarm initialized: current node (umqocsykyg1xxsn6wvk4igx1k) is now a manager.To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-5070xwlej15dop8d8d29do3l01z6tsfwyjwnkvi9pil0qubj8g-3fwsm8707uwu5yeb6n2o6ilpk 172.31.22.17:2377To add a manager to this swarm, run &apos;docker swarm join-token manager&apos; and follow the instructions. 현재의 서버가 매니저가 되었다. 이제 관리할 worker들을 swarm에 join 시켜야 하는데, 위 커맨드의 실행결과에 나온 Token을 사용하여 worker node에서 실행 시켜 주면 swarm에 join 하게 된다. 더불어 복수의 manager node를 운영하고 싶을 경우 추가할 수 있는데 이 부분은 쿼럼 설명 때 자세히 하겠다. 일반적으로 Token은 공용 저장소등을 이용해 관리하게 되는데 잃어 버리더라고 당황하지 말자. 아래 명령을 이용하면 해당 Swarm에서 발급한 Token을 확인 할 수 있다. 1234[root@test1 ~]# docker swarm join-token workerTo add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-5070xwlej15dop8d8d29do3l01z6tsfwyjwnkvi9pil0qubj8g-3fwsm8707uwu5yeb6n2o6ilpk 172.31.22.17:2377 추후 manager node를 추가 할 때는 아래와 같이 추가 manager node에 발급되는 Token을 확인 할 수 있다. 자세히 보면 토큰의 “-“ 뒷부분의 code가 worker와 다른 것을 확인 할 수 있다. 1234[root@test1 ~]# docker swarm join-token managerTo add a manager to this swarm, run the following command: docker swarm join --token SWMTKN-1-5070xwlej15dop8d8d29do3l01z6tsfwyjwnkvi9pil0qubj8g-8j43t1kqsmbpo68eulvgvgahz 172.31.22.17:2377 Add nodes현재 Swarm이 관리하고 있는 서버 현황을 살펴보자. 123[root@test1 ~]# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUSumqocsykyg1xxsn6wvk4igx1k * test1 Ready Active Leader 현재로써는 manager node인 test1 만 등록되어 있는것을 확인 할 수 있다. test1 node의 ID 값에 “*” 표시는 특별한 Node라는 표시이며 MANAGER STATUS에서 “Leader” 로 지정된 것을 확인 할 수 있다. 이제 worker node를 여기에 추가해보자. 우선 새로운 서버(test2)에서 위 1.1번에서 진행했던 방식대로 docker를 설치한다. 12[root@test2 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE 자 이제 그럼 test1번 서버에서 worker를 join 시키기위한 Token을 다시 한번 확인 해보자 1234[root@test1 ~]# docker swarm join-token workerTo add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-5070xwlej15dop8d8d29do3l01z6tsfwyjwnkvi9pil0qubj8g-3fwsm8707uwu5yeb6n2o6ilpk 172.31.22.17:2377 실행된 결과에서 “docker swarm join –token SWMTKN-1-….” 라인을 copy하여 test2 와 test3 서버에서 실행한다. 1234567[root@test2 ~]# docker swarm join --token SWMTKN-1-5070xwlej15dop8d8d29do3l01z6tsfwyjwnkvi9pil0qubj8g-3fwsm8707uwu5yeb6n2o6ilpk 172.31.22.17:2377This node joined a swarm as a worker.[root@test3 ~]# docker swarm join --token SWMTKN-1-5070xwlej15dop8d8d29do3l01z6tsfwyjwnkvi9pil0qubj8g-3fwsm8707uwu5yeb6n2o6ilpk 172.31.22.17:2377This node joined a swarm as a worker 실행 결과를 보면 worker로 swarm에 조인되었다는 메시지를 확인 할 수 있다. 실제로 swarm에 등록 되었는지 test1에서 확인해보자. 1234567891011121314[root@test1 ~]# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUSumqocsykyg1xxsn6wvk4igx1k * test1 Ready Active Leaderrugmetip3d1il2cokfqt1bx03 test2 Ready Activezl4ujrl7uh38saoyu0r0i42pz test3 Ready Active[root@test1 ~]# docker system info | grep -A 6 SwarmSwarm: active NodeID: umqocsykyg1xxsn6wvk4igx1k Is Manager: true ClusterID: 5j843ntmj4xr8djib232rekze Managers: 1 Nodes: 3 Orchestration: 위와 같이 node가 추가 된 것을 확인 할 수 있다. Swarm Backup and Restoreswarm manager node에서 httpd docker image를 하나 다운로드 받아 두자 1234[root@test1 ~]# docker pull httpd[root@test1 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEhttpd latest 7239615c0645 3 weeks ago 177MB 이제 swarm 클러스터의 woker node에 web service를 배포해보자 서비스 배포는 아래의 명령으로 수행 할 수있는 데 각 파라메터의 의미는 서비스 name은 test_web으로 서비스 port는 80, 그리고 클러스터내에 최소한 2개 이상의 node에 배포하고 docker image는 httpd를 사용한다는 의미이다. 123456[root@test1 ~]# docker service create --name test_web --publish 80:80 --replicas 2 httpduj4blgm7ni3huwp0zuc36fysroverall progress: 2 out of 2 tasks1/2: running2/2: runningverify: Service converged 자 이제 web 서비스가 2대의 worker node에 배포가 되었다. 실제 배포가 잘되었는지 서비스 현황이 어떤지 다음의 명령으로 확인한다. 12345678[root@test1 ~]# docker service lsID NAME MODE REPLICAS IMAGE PORTSuj4blgm7ni3h test_web replicated 2/2 httpd:latest *:80-&gt;80/tcp[root@test1 ~]# docker service ps test_webID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS0s1c0k4ptuk3 test_web.1 httpd:latest test2 Running Running 5 minutes agonozonu306ltq test_web.2 httpd:latest test1 Running Running 5 minutes ago test_web 서비스가 test1, 2번 node에 배포되었고 실행중인것을 확인 할 수 있고, 실제 배포된 worker node 1번, 2번에 로그인 해서 상황을 봐도 잘 수행된것을 확인 할 수 있다. 12345678[root@test1 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES36df35af0ac3 httpd:latest &quot;httpd-foreground&quot; 8 minutes ago Up 7 minutes 80/tcp test_web.2.nozonu306ltq4pgwca76ar2e3[root@test2 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESfdca636fcbea httpd:latest &quot;httpd-foreground&quot; 7 minutes ago Up 7 minutes 80/tcp test_web.1.0s1c0k4ptuk3bcwak4pngxuux Swarm Backupswarm의 backup과 restore는 생각 보다 간단한데, swarm Cluster를 관리하는 모든 파일은 /var/lib/docker/swarm 디렉토리에 위치한다. 이 디렉토리를 별도의 공간에 주기적으로 backup하고 문제 발생시 디렉토리를 overwrite 해주면 문제 발생 이전 상황으로 되돌 릴 수 있다. 1234567[root@test1 ~]# ll /var/lib/docker/swarm/total 8drwxr-xr-x. 2 root root 72 Jan 7 05:59 certificates-rw-------. 1 root root 149 Jan 7 05:59 docker-state.jsondrwx------. 4 root root 53 Jan 7 05:59 raft-rw-------. 1 root root 68 Jan 7 05:59 state.jsondrwxr-xr-x. 2 root root 21 Jan 7 05:59 worker 파일들을 백업하기 위해 docker service를 중지하고 파일을 적정한 디렉토리로 copy 한 후 다시 docker service를 start 한다. 12345678910[root@test1 ~]# systemctl stop docker[root@test1 ~]# cp -rf /var/lib/docker/swarm/ .[root@test1 ~]# ll ./swarm/total 8drwxr-xr-x. 2 root root 72 Jan 7 07:09 certificates-rw-------. 1 root root 149 Jan 7 07:09 docker-state.jsondrwx------. 4 root root 53 Jan 7 07:09 raft-rw-------. 1 root root 68 Jan 7 07:09 state.jsondrwxr-xr-x. 2 root root 21 Jan 7 07:09 worker[root@test1 ~]# systemctl start docker 백업된 파일은 tar로 압축하여 적절한 원격지 서버로 copy 해 둔다. 12[root@test1 ~]# tar cvzf swarm_backup_20171010.tar.gz ./swarm[root@test1 ~]# scp swarm_backup_20171010.tar.gz user@test2:/backup Swarm Restore자 이제 manager node가 crash 되어서 복구 불가하다고 가정 해보자. 이 상태에서는 새로운 Manager 서버를 만들어 swarm Cluster 를 복구 해야 할 것 이다. 이런 상황을 연출하기 위해 Swarm 내 모든 서버들의 docker 서비스를 중지 해보자. 이제 swarm cluster는 완전히 서비스가 중지된 상태이다. 123[root@test1 ~]# systemctl stop docker[root@test2 ~]# systemctl stop docker[root@test3 ~]# systemctl stop docker 먼저 새로운 manager node가 될 서버(test4)에 docker를 설치하고, /var/lib/docker/swarm/ 디렉토리를 삭제한 후 백업된 swarm file를 압축을 풀어 동일한 디렉토리에 copy 하자. 12345678910111213141516171819202122[root@test4 ~]# systemctl stop docker[root@test4 ~]# rm -rf /var/lib/docker/swarm/[root@test4 ~]# ls /var/lib/docker/builder containers image plugins tmp volumescontainerd devicemapper network runtimes trust[root@test4 ~]# tar xvzf /tmp/swarm_backup_20171010.tar.gz -C /var/lib/docker/./swarm/./swarm/certificates/./swarm/certificates/swarm-root-ca.crt./swarm/certificates/swarm-node.key./swarm/certificates/swarm-node.crt./swarm/docker-state.json./swarm/worker/./swarm/worker/tasks.db./swarm/raft/./swarm/raft/snap-v3-encrypted/./swarm/raft/wal-v3-encrypted/./swarm/raft/wal-v3-encrypted/0000000000000000-0000000000000000.wal./swarm/state.json[root@test4 ~]# ls /var/lib/docker/builder containers image plugins swarm trustcontainerd devicemapper network runtimes tmp volumes 이제 restore된 swarm 설정 파일로 docker를 다시 수행 하고 swarm cluster를 initialize 해보자 1234567[root@test4 ~]# systemctl start docker[root@test4 ~]# docker swarm init --force-new-clusterSwarm initialized: current node (umqocsykyg1xxsn6wvk4igx1k) is now a manager.To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-5070xwlej15dop8d8d29do3l01z6tsfwyjwnkvi9pil0qubj8g-3fwsm8707uwu5yeb6n2o6ilpk 172.31.22.17:2377To add a manager to this swarm, run &apos;docker swarm join-token manager&apos; and follow the instructions. test4번이 새로운 manager가 되었다. 더불어 worker node를 추가 하기 위한 Token이 새로 생성된것이 아니라 이전에 사용하던 것임을 확인 할 수 있다. 그럼 이전에 만들어 놓았다 Test_web 서비스가 살아 있는지 확인해보자 12345[root@test4 ~]# docker service lsID NAME MODE REPLICAS IMAGE PORTSuj4blgm7ni3h test_web replicated 2/2 httpd:latest *:80-&gt;80/tcp 위와 같이 test_web 서비스가 2개의 복제본을 가지고 제대로 올라온 것을 확인 할 수 있다. 대상 worker node도 2번과 4번 을 지정했으나 2번 node가 down 상태이므로 4번 node에 2개를 모두 생성한 것을 확인 할 수 있다. 123456789101112131415[root@test4 ~]# docker service ps test_webID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSasi19jawpbrg test_web.1 httpd:latest test4 Running Running 3 minutes ago kj8kzmnmf7yu \\_ test_web.1 httpd:latest test4 Shutdown Complete 3 minutes ago 0s1c0k4ptuk3 \\_ test_web.1 httpd:latest test2 Shutdown Running about an hour ago 3ah4p185dwbx test_web.2 httpd:latest test4 Running Running 3 minutes ago tvbs2saru75x \\_ test_web.2 httpd:latest test4 Shutdown Complete 3 minutes ago nozonu306ltq \\_ test_web.2 httpd:latest test4 Shutdown Failed 5 minutes ago &quot;No such container: test_web.2…&quot; Outline the sizing requirements prior to installationCluster Sizing requirements CPU, Memory and Disk Concurrency UCP(Universal Control Plane) System Requirements Breakdown of Manager nodes for Fault Tolerance Minimum Requirements 8GB RAM (Manger or DTR nodes) 4GB RAM (Workers) 3GB Free Disk Recommended Requirement 16GB RAM( Manager or DTR Nodes) 4vCPUs (Workers or DTR Nodes) 25-100GB Disk Space Performance Consideration (Timing) Special Notes Docker EE includes Docker Engine with support Docker Trusted Registry Universal Control Plane Compatibility Docker Engine 17.06+ DTR 2.3+ UCP 2.2+ Recommendations plan for Load Balancing User External Certificate Authority for production Setup and Configure Universal Control Plane(UCP) and Docker Trusted Repository(DTR) for Secure Cluster Managementinstall UCP1234567891011121314151617181920212223242526[root@test1 ~]# docker container run --rm -it --name ucp -v /var/run/docker.sock:/var/run/docker.sock docker/ucp:2.2.4 install --host-address 172.31.22.17 --interactiveUnable to find image &apos;docker/ucp:2.2.4&apos; locally2.2.4: Pulling from docker/ucpb56ae66c2937: Pull complete68de5ce90bd1: Pull completed2de1fff8042: Pull completeDigest: sha256:2b819b92d2209c0a5680fcee3f99c1090a9d4f1e6fea3003a9a661cbd16cc851Status: Downloaded newer image for docker/ucp:2.2.4INFO[0000] Verifying your system is compatible with UCP 2.2.4 (168ec746e)INFO[0000] Your engine version 17.12.0-ce, build c97c6d6 (3.10.0-693.11.1.el7.x86_64) is compatibleAdmin Username: adminAdmin Password:Confirm Admin Password:INFO[0053] Pulling required images... (this may take a while)INFO[0053] Pulling docker/ucp-agent:2.2.4INFO[0067] Pulling docker/ucp-auth:2.2.4INFO[0084] Pulling docker/ucp-dsinfo:2.2.4INFO[0135] Pulling docker/ucp-compose:2.2.4INFO[0160] Pulling docker/ucp-controller:2.2.4INFO[0176] Pulling docker/ucp-etcd:2.2.4INFO[0192] Pulling docker/ucp-cfssl:2.2.4INFO[0201] Pulling docker/ucp-metrics:2.2.4INFO[0209] Pulling docker/ucp-swarm:2.2.4INFO[0217] Pulling docker/ucp-auth-store:2.2.4INFO[0223] Pulling docker/ucp-hrm:2.2.4WARN[0229] None of the hostnames we&apos;ll be using in the UCP certificates [test1 127.0.0.1 172.17.0.1 172.31.22.17] contain a domain component. Your generated certs may fail TLS validation unless you only use one of these shortnames or IPs to connect. You can use the --san flag to add more aliases addtional aliaese 12345678910111213Additional aliases: ucp.mytest.comINFO[0009] Establishing mutual Cluster Root CA with SwarmINFO[0012] Installing UCP with host address 172.31.22.17 - If this is incorrect, please specify an alternative address with the &apos;--host-address&apos; flagINFO[0012] Generating UCP Client Root CAINFO[0012] Deploying UCP ServiceINFO[0053] Installation completed on test1 (node umqocsykyg1xxsn6wvk4igx1k)INFO[0059] Installation completed on test2 (node rugmetip3d1il2cokfqt1bx03)INFO[0063] Installation completed on test3 (node zl4ujrl7uh38saoyu0r0i42pz)INFO[0063] UCP Instance ID: 5j843ntmj4xr8djib232rekzeINFO[0063] UCP Server SSL: SHA-256 Fingerprint=83:50:09:06:1D:A0:3C:B5:D6:D4:99:BD:D1:83:71:60:C9:CD:F4:F5:14:AE:5B:9D:37:F9:F9:15:B6:01:80:A5INFO[0063] Login to UCP at https://172.31.22.17:443INFO[0063] Username: adminINFO[0063] Password: (your admin password) 12345678910111213[root@test1 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES7dd45912f607 docker/ucp-agent:2.2.4 &quot;/bin/ucp-agent agent&quot; About a minute ago Up About a minute 2376/tcp ucp-agent.umqocsykyg1xxsn6wvk4igx1k.gtmbpboifov5ggqf2im5qz2n948a3e982d35a docker/ucp-controller:2.2.4 &quot;/bin/controller ser…&quot; About a minute ago Up About a minute (healthy) 0.0.0.0:443-&gt;8080/tcp ucp-controllerabc9f112aef1 docker/ucp-auth:2.2.4 &quot;/usr/local/bin/enzi…&quot; About a minute ago Up About a minute (healthy) 0.0.0.0:12385-&gt;4443/tcp ucp-auth-api67faa78d9df0 docker/ucp-metrics:2.2.4 &quot;/bin/entrypoint.sh …&quot; About a minute ago Up About a minute (healthy) 0.0.0.0:12387-&gt;12387/tcp ucp-metricsc6bb9064aaec docker/ucp-swarm:2.2.4 &quot;/bin/swarm manage -…&quot; About a minute ago Up About a minute (healthy) 0.0.0.0:2376-&gt;2375/tcp ucp-swarm-managerdf59324663aa docker/ucp-auth:2.2.4 &quot;/usr/local/bin/enzi…&quot; About a minute ago Up About a minute (healthy) 0.0.0.0:12386-&gt;4443/tcp ucp-auth-workerbb818ffa3d7e docker/ucp-auth-store:2.2.4 &quot;rethinkdb --bind al…&quot; About a minute ago Up About a minute (healthy) 0.0.0.0:12383-12384-&gt;12383-12384/tcp ucp-auth-store38fbbd11781a docker/ucp-etcd:2.2.4 &quot;/bin/etcd --data-di…&quot; About a minute ago Up About a minute (healthy) 2380/tcp, 4001/tcp, 7001/tcp, 0.0.0.0:12380-&gt;12380/tcp, 0.0.0.0:12379-&gt;2379/tcp ucp-kv0cacfcca1d86 docker/ucp-cfssl:2.2.4 &quot;/bin/ucp-ca serve -…&quot; About a minute ago Up About a minute (healthy) 0.0.0.0:12381-&gt;12381/tcp ucp-cluster-root-cac96b8588e400 docker/ucp-cfssl:2.2.4 &quot;/bin/ucp-ca serve -…&quot; About a minute ago Up About a minute (healthy) 0.0.0.0:12382-&gt;12382/tcp ucp-client-root-caaec58234b750 docker/ucp-agent:2.2.4 &quot;/bin/ucp-agent prox…&quot; About a minute ago Up About a minute (healthy) 0.0.0.0:12376-&gt;2376/tcp ucp-proxy UCP web site 접속 license는 스킵 login을 하게 되면 UCP의 Dashboard 화면을 볼 수 있다. node 수라던지 images 개수, Container의 현황등 여러 확인 할 수 있다. woker node들에서 container 현황을 보면 UCP agent 들이 실행되어 있는 것을 확인 할 수 있다. 12345[user@test3 ~]$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES0a03ad0b88b8 docker/ucp-agent:2.2.4 &quot;/bin/ucp-agent agent&quot; 39 seconds ago Up 34 seconds 2376/tcp ucp-agent.zl4ujrl7uh38saoyu0r0i42pz.k3ecm199utjg99a5k1dfefccqeeae90c5eed8 docker/ucp-agent:2.2.4 &quot;/bin/ucp-agent prox…&quot; About an hour ago Up About an hour (healthy) 0.0.0.0:12376-&gt;2376/tcp ucp-proxy 아 이제 DTR (Docker Trusted Registry)를 셋업할 준비를 하자 UCP의 왼쪽 네비게이션 메뉴중에 admin -&gt; admin Settings로 들어간다. 다시 왼쪽 네이비게이션 메뉴에서 Docker Trusted Registry 메뉴를 선택하면 오른쪽 화면에 관련 설정을 할 수 있는 메뉴들이 나온다. DTR 서비스를 제공할 Node를 선택한다. 여기서는 test2로 지정했다 TLS 인증은 disable 한다. 설정을 완료하면 하단부에 수행 해야할 command line 명령어가 생성 되면 copy to clipboard 버튼을 클릭하여 클립보드로 명령어 라인을 copy한다 다시 UCP 서버인 test1의 command line에서 copy한 command를 아래와 같이 수행 한다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657[root@test1 ~]# docker run -it --rm docker/dtr install --ucp-node test2 --ucp-username admin --ucp-url https://ucp.mytest.com --ucp-insecure-tlsINFO[0000] Beginning Docker Trusted Registry installation ucp-password: INFO[0003] Validating UCP cert INFO[0003] Connecting to UCP INFO[0004] The UCP cluster contains the following nodes without port conflicts: test2, test3 INFO[0004] Searching containers in UCP for DTR replicas INFO[0004] Searching containers in UCP for DTR replicas INFO[0004] verifying [80 443] ports on test2 INFO[0010] starting phase 2 INFO[0000] Validating UCP cert INFO[0000] Connecting to UCP INFO[0000] Verifying your system is compatible with DTR INFO[0000] Checking if the node is okay to install on INFO[0000] Creating network: dtr-ol INFO[0000] Connecting to network: dtr-ol INFO[0000] Waiting for phase2 container to be known to the Docker daemon INFO[0000] Starting UCP connectivity test INFO[0000] UCP connectivity test passed INFO[0000] Setting up replica volumes... INFO[0001] Creating initial CA certificates INFO[0001] Bootstrapping rethink... INFO[0001] Creating dtr-rethinkdb-993d3a693f09... INFO[0029] Establishing connection with Rethinkdb INFO[0030] Waiting for database dtr2 to exist INFO[0030] Establishing connection with Rethinkdb INFO[0034] Generated TLS certificate. dnsNames=[*.com *.*.com example.com *.dtr *.*.dtr] domains=[*.com *.*.com 172.17.0.1 example.com *.dtr *.*.dtr] ipAddresses=[172.17.0.1]INFO[0035] License config not copied from UCP because UCP has no valid license. INFO[0035] Migrating db... INFO[0000] Establishing connection with Rethinkdb INFO[0000] Migrating database schema fromVersion=0 toVersion=8INFO[0004] Waiting for database notaryserver to exist INFO[0005] Waiting for database notarysigner to exist INFO[0005] Waiting for database jobrunner to exist INFO[0007] Migrated database from version 0 to 8 INFO[0042] Starting all containers... INFO[0042] Getting container configuration and starting containers... INFO[0043] Recreating dtr-rethinkdb-993d3a693f09... INFO[0048] Creating dtr-registry-993d3a693f09... INFO[0065] Creating dtr-garant-993d3a693f09... INFO[0090] Creating dtr-api-993d3a693f09... INFO[0116] Creating dtr-notary-server-993d3a693f09... INFO[0135] Recreating dtr-nginx-993d3a693f09... INFO[0160] Creating dtr-jobrunner-993d3a693f09... INFO[0203] Creating dtr-notary-signer-993d3a693f09... INFO[0211] Creating dtr-scanningstore-993d3a693f09... INFO[0221] Trying to get the kv store connection back after reconfigure INFO[0221] Establishing connection with Rethinkdb INFO[0222] Verifying auth settings... INFO[0222] Successfully registered dtr with UCP INFO[0222] Establishing connection with Rethinkdb INFO[0223] Background tag migration started INFO[0223] Installation is complete INFO[0223] Replica ID is set to: 993d3a693f09 INFO[0223] You can use flag &apos;--existing-replica-id 993d3a693f09&apos; when joining other replicas to your Docker Trusted Registry Cluster 인스톨이 끝났으면 container 현황을 확인하면 DTR에 관련된 여러 container가 수행중인것을 확인 할 수 있다. 12345678910111213141516171819202122232425[root@test2 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESc13dac6b7678 docker/ucp-agent:2.2.4 &quot;/bin/ucp-agent agent&quot; 8 seconds ago Up 2 seconds 2376/tcp ucp-agent.rugmetip3d1il2cokfqt1bx03.jltaf9xeqb7cs00jd5uyeywopc78ce0d67529 docker/dtr-postgres:2.4.1 &quot;/bin/postgreswrapper&quot; 31 seconds ago Up 29 seconds 5432/tcp dtr-scanningstore-993d3a693f09d09e4f033da9 docker/dtr-notary-signer:2.4.1 &quot;/start.sh&quot; 41 seconds ago Up 39 seconds dtr-notary-signer-993d3a693f09b00889f09c53 docker/dtr-jobrunner:2.4.1 &quot;/bin/jobrunner work…&quot; 50 seconds ago Up 48 seconds dtr-jobrunner-993d3a693f0903106001a3e6 docker/dtr-nginx:2.4.1 &quot;/bin/nginxwrapper&quot; About a minute ago Up 28 seconds 0.0.0.0:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp dtr-nginx-993d3a693f09d73ede5a13cc docker/dtr-notary-server:2.4.1 &quot;/start.sh&quot; About a minute ago Up About a minute dtr-notary-server-993d3a693f09a95c142f6ee2 docker/dtr-api:2.4.1 &quot;/bin/api&quot; 2 minutes ago Up 2 minutes dtr-api-993d3a693f091876fd8f345b docker/dtr-garant:2.4.1 &quot;/bin/garant&quot; 2 minutes ago Up 2 minutes dtr-garant-993d3a693f09a4e11b70ff64 docker/dtr-registry:2.4.1 &quot;/bin/registry&quot; 3 minutes ago Up 3 minutes dtr-registry-993d3a693f09492d36ae37aa docker/dtr-rethink:2.4.1 &quot;/bin/rethinkwrapper&quot; 3 minutes ago Up 3 minutes dtr-rethinkdb-993d3a693f093250066f27ce docker/ucp-agent:2.2.4 &quot;/bin/ucp-agent prox…&quot; About an hour ago Up About an hour (healthy) 0.0.0.0:12376-&gt;2376/tcp ucp-proxy 자 이제 dtr.mytest.com으로 접속해보자 접속하는 과정은 UCP와 동일하다 Docker Repository 관리 화면을 볼 수 있다. Backup UCP &amp; DTRd 1234[root@test1 ~]# docker container run --log-driver none --rm -i --name ucp -v /var/run/docker.sock:/var/run/docker.sock docker/ucp backup &gt; backup.tarINFO[0000] Your engine version 17.12.0-ce, build c97c6d6 (3.10.0-693.11.1.el7.x86_64) is compatibleINFO[0000] We detected local components of UCP instance 5j843ntmj4xr8djib232rekzeFATA[0000] re-run the command with &quot;--id 5j843ntmj4xr8djib232rekze&quot; or --interactive to confirm you want to backup this UCP instance 위와 같은 에러가 발생하는 이유는 id 가 지정되지 않아서 인데, error 메시지의 id 값을 copy하여 다시 수행한다. 12345678[root@test1 ~]# docker container run --log-driver none --rm -i --name ucp -v /var/run/docker.sock:/var/run/docker.sock docker/ucp backup --id 5j843ntmj4xr8djib232rekze &gt; backup.tarINFO[0000] Your engine version 17.12.0-ce, build c97c6d6 (3.10.0-693.11.1.el7.x86_64) is compatibleINFO[0000] Temporarily stopping local UCP containers to ensure a consistent backupINFO[0061] Backing up internal KV storeINFO[0000] Beginning backupWARN[0000] Unable to copy license file from UCP controller container - continuing without license backup: Error response from daemon: Could not find the file /etc/ucp/docker.lic in container 48a3e982d35ac13d6596e24f75c6a285fd33eba8c7b0274f817bdf0dc6028866INFO[0001] Backup completed successfullyINFO[0066] Resuming stopped UCP containers ucp에 관련된 모든 내용들이 backup.tar로 백업되었고 용량은 171MB 가 되는 것을 확인할 수 있다. backup command를 clone job에 등록해서 정기적으로 UCP 내용을 백업 받는 것을 권장한다. 123456[root@test1 ~]# ls -lshtotal 2.2G171M -rw-r--r--. 1 root root 171M Jan 7 13:26 backup.tar2.0G -rw-r--r--. 1 root root 2.0G Jan 7 2015 swap 0 drwx------. 5 root root 90 Jan 7 07:09 swarm100K -rw-r--r--. 1 root root 97K Jan 7 07:15 swarm_backup_20171010.tar.gz 그러면 Restore를 어떻게 할까? 단순하다 아래와 같이 백업된 파일을 대상으로 Restore하는 명령으로 수행 할 수 있다. 1[root@test1 ~]# docker container run --log-driver none --rm -i --name ucp -v /var/run/docker.sock:/var/run/docker.sock docker/ucp restore --id 5j843ntmj4xr8djib232rekze &lt; backup.tar Backup DTR123456789101112131415[root@test2 ~]# docker run -i --rm docker/dtr backup --ucp-insecure-tls --ucp-url https://dtr.mytest.com:443 --ucp-username admin --ucp-password passwordhadk &gt; dtr-backup.tarINFO[0000] Validating UCP cert INFO[0000] Connecting to UCP INFO[0000] Searching containers in UCP for DTR replicas INFO[0000] This cluster contains the replicas: 993d3a693f09 Choose a replica to back up from [993d3a693f09]: INFO[0000] Validating UCP cert INFO[0000] Connecting to UCP INFO[0000] Connecting to network: dtr-ol INFO[0000] Waiting for phase2 container to be known to the Docker daemon INFO[0000] Establishing connection with Rethinkdb INFO[0000] Backing up DTR INFO[0000] Establishing connection with Rethinkdb INFO[0011] Backup complete. 1234[root@test2 ~]# ls -lshtotal 2.1G 92K -rw-r--r--. 1 root root 92K Jan 7 13:39 dtr-backup.tar2.0G -rw-r--r--. 1 root root 2.0G Jan 7 2015 swap restore 12[root@test2 ~]# docker run -i --rm docker/dtr restore --ucp-insecure-tls --ucp-url https://dtr.mytest.com:443 --ucp-username admin --ucp-password passwordhadk &gt; dtr-backup.tar Add User Docker and Name spacesNamespace Types Process ID Mount IPC User(currently experimental support for) Networks Docker and Control Groups(cgroups)Common control Groups CPU Memory Network Bandwidth Disk Priority","categories":[{"name":"Container","slug":"Container","permalink":"http://yoursite.com/categories/Container/"},{"name":"Docker","slug":"Container/Docker","permalink":"http://yoursite.com/categories/Container/Docker/"}],"tags":[]}]}